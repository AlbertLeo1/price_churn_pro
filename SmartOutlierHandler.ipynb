{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed98b06d-cdd2-44bb-bbb9-056dabf9b0b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     24\u001b[39m         check(estimator)  \n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mSmartOutlierHandler\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mTransformerMixin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBaseEstimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43miqr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mOptional\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mSmartOutlierHandler\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSmartOutlierHandler\u001b[39;00m(TransformerMixin, BaseEstimator):\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     28\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     29\u001b[39m         method: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33miqr\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     30\u001b[39m         replace_strategy: \u001b[38;5;28mstr\u001b[39m = \u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     31\u001b[39m         border_quantile: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.01\u001b[39m,\n\u001b[32m     32\u001b[39m         contamination: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.05\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         threshold: \u001b[43mOptional\u001b[49m[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     34\u001b[39m         min_std: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m1e-3\u001b[39m,\n\u001b[32m     35\u001b[39m         default_value: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m     36\u001b[39m         verbose: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     37\u001b[39m         columns: Optional[List[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     38\u001b[39m         max_components: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m,\n\u001b[32m     39\u001b[39m         min_peak_height_ratio: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.05\u001b[39m,\n\u001b[32m     40\u001b[39m         random_state: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     41\u001b[39m     ):\n\u001b[32m     43\u001b[39m         \u001b[38;5;28mself\u001b[39m.method = method\n\u001b[32m     44\u001b[39m         \u001b[38;5;28mself\u001b[39m.replace_strategy = replace_strategy\n",
      "\u001b[31mNameError\u001b[39m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.signal import find_peaks\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.validation import check_is_fitted, check_array\n",
    "from typing import List, Union\n",
    "import warnings\n",
    "from sklearn.utils.estimator_checks import check_estimator \n",
    "from sklearn.utils.estimator_checks import _yield_all_checks\n",
    "\n",
    "        \n",
    "def check_my_estimator(estimator):\n",
    "    # skip test for nan and inf checks because your transformer allows them\n",
    "    skipped = ['check_estimators_nan_inf']\n",
    "    for check in _yield_all_checks(estimator):\n",
    "        if check.func.__name__ in skipped:\n",
    "            continue\n",
    "        check(estimator)  \n",
    "\n",
    "class SmartOutlierHandler(TransformerMixin, BaseEstimator):\n",
    "    def __init__(\n",
    "        self,\n",
    "        method: str = 'iqr',\n",
    "        replace_strategy: str = 'median',\n",
    "        border_quantile: float = 0.01,\n",
    "        contamination: float = 0.05,\n",
    "        threshold: Optional[float] = None,\n",
    "        min_std: float = 1e-3,\n",
    "        default_value: float = 0,\n",
    "        verbose: bool = False,\n",
    "        columns: Optional[List[Union[str, int]]] = None,\n",
    "        max_components: int = 5,\n",
    "        min_peak_height_ratio: float = 0.05,\n",
    "        random_state: Optional[int] = None\n",
    "    ):\n",
    "\n",
    "        self.method = method\n",
    "        self.replace_strategy = replace_strategy\n",
    "        self.border_quantile = border_quantile\n",
    "        self.contamination = contamination\n",
    "        self.threshold = threshold\n",
    "        self.min_std = min_std\n",
    "        self.default_value = default_value\n",
    "        self.verbose = verbose\n",
    "        self.columns = columns\n",
    "        self.max_components = max_components\n",
    "        self.min_peak_height_ratio = min_peak_height_ratio\n",
    "        self.random_state = random_state\n",
    "            \n",
    "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y=None):\n",
    "        X = check_array(X, ensure_2d=True, ensure_all_finite='allow-nan') \n",
    "        #X = check_array(X, ensure_2d=True)\n",
    "        if np.iscomplexobj(X):\n",
    "            raise ValueError(\"Complex data not supported\")\n",
    "        \"\"\"Fit the outlier handler to the data.\"\"\"\n",
    "        # Convert to DataFrame if not already\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "            generated_cols = True\n",
    "        else:\n",
    "            generated_cols = False\n",
    "            \n",
    "        # Store feature information\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        self.feature_names_in_ = np.asarray(X.columns, dtype=object) if not generated_cols else None\n",
    "        self.dtypes_ = X.dtypes.to_dict()\n",
    "        \n",
    "        # If columns not specified and we have generated columns, use all numeric columns\n",
    "        if self.columns is None:\n",
    "            if generated_cols:\n",
    "                self.columns_ = list(range(X.shape[1]))  # Use all columns for array input\n",
    "            else:\n",
    "                self.columns_ = [i for i, col in enumerate(X.columns) \n",
    "                               if pd.api.types.is_numeric_dtype(X.iloc[:, i])]\n",
    "        else:\n",
    "            self.columns_ = []\n",
    "            for col in self.columns:\n",
    "                if isinstance(col, int) and col < X.shape[1]:\n",
    "                    self.columns_.append(col)\n",
    "                elif isinstance(col, str) and col in X.columns:\n",
    "                    self.columns_.append(X.columns.get_loc(col))\n",
    "        \n",
    "        if not self.columns_:\n",
    "            raise ValueError(\"No valid numeric columns found to process\")\n",
    "        \n",
    "        # Rest of your original fit logic...\n",
    "        self.clip_params_ = {} \n",
    "        \n",
    "        for col_idx in self.columns_:\n",
    "            col_name = X.columns[col_idx]\n",
    "            series = X.iloc[:, col_idx].dropna()\n",
    "            \n",
    "            if len(series) < 2 or series.nunique() < 2:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nüìå [Column {col_name}] Skipped - insufficient data (n={len(series)}, unique={series.nunique()})\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                if self.verbose:\n",
    "                    print(\"\\n\" + \"-\"*40)\n",
    "                    print(f\"üîß Processing column: {col_name}\")\n",
    "                    print(f\"üìà Data stats: mean={series.mean():.2f}, std={series.std():.2f}\")\n",
    "                    print(f\"üìä Min={series.min():.2f}, 25%={series.quantile(0.25):.2f}, \"\n",
    "                          f\"50%={series.median():.2f}, 75%={series.quantile(0.75):.2f}, \"\n",
    "                          f\"Max={series.max():.2f}\")\n",
    "                    print(f\"üî¢ Non-null values: {len(series)}\")\n",
    "                \n",
    "                if self.method == 'gmm':\n",
    "                    if self.verbose:\n",
    "                        print(\"\\nüîÆ Fitting Gaussian Mixture Model...\")\n",
    "                    model, clip_info = self._fit_gmm(series)\n",
    "                    self.clip_params_[col_idx] = {'model': model, 'clip_info': clip_info}\n",
    "                    if self.verbose:\n",
    "                        print(f\"‚úÖ GMM fitted with {model.n_components} components\")\n",
    "                        for cluster, bounds in clip_info.items():\n",
    "                            print(f\"   Cluster {cluster}: bounds=[{bounds['low']:.2f}, {bounds['high']:.2f}]\")\n",
    "                elif self.method == 'iqr':\n",
    "                    Q1, Q3 = series.quantile([0.25, 0.75])\n",
    "                    IQR = Q3 - Q1\n",
    "                    self.clip_params_[col_idx] = {\n",
    "                        'low': Q1 - 1.5 * IQR,\n",
    "                        'high': Q3 + 1.5 * IQR\n",
    "                    }\n",
    "                    if self.verbose:\n",
    "                        print(\"\\nüìè Using IQR method:\")\n",
    "                        print(f\"   Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "                        print(f\"   Lower bound: {Q1 - 1.5*IQR:.2f}\")\n",
    "                        print(f\"   Upper bound: {Q3 + 1.5*IQR:.2f}\")\n",
    "                elif self.method == 'clip':\n",
    "                    low = series.quantile(self.border_quantile)\n",
    "                    high = series.quantile(1 - self.border_quantile)\n",
    "                    self.clip_params_[col_idx] = {'low': low, 'high': high}\n",
    "                    if self.verbose:\n",
    "                        print(\"\\n‚úÇÔ∏è Using quantile clipping:\")\n",
    "                        print(f\"   Lower bound ({self.border_quantile*100}%): {low:.2f}\")\n",
    "                        print(f\"   Upper bound ({(1-self.border_quantile)*100}%): {high:.2f}\")\n",
    "                elif self.method == 'isolation_forest':\n",
    "                    if self.verbose:\n",
    "                        print(\"\\nüå≤ Fitting Isolation Forest...\")\n",
    "                    iso = IsolationForest(\n",
    "                        contamination=self.contamination, \n",
    "                        random_state=42\n",
    "                    )\n",
    "                    iso.fit(series.values.reshape(-1, 1))\n",
    "                    self.clip_params_[col_idx] = {\n",
    "                        'model': iso,\n",
    "                        'low': series.quantile(self.border_quantile),\n",
    "                        'high': series.quantile(1 - self.border_quantile)\n",
    "                    }\n",
    "                    if self.verbose:\n",
    "                        print(f\"‚úÖ Isolation Forest fitted with contamination={self.contamination}\")\n",
    "                        print(f\"   Fallback bounds: [{self.clip_params_[col_idx]['low']:.2f}, \"\n",
    "                              f\"{self.clip_params_[col_idx]['high']:.2f}]\")\n",
    "                \n",
    "                if self.verbose and self.method in ['iqr', 'clip', 'isolation_forest']:\n",
    "                    bounds = self.clip_params_[col_idx]\n",
    "                    n_outliers_low = (series < bounds['low']).sum()\n",
    "                    n_outliers_high = (series > bounds['high']).sum()\n",
    "                    total_outliers = n_outliers_low + n_outliers_high\n",
    "                    outlier_pct = total_outliers / len(series) * 100\n",
    "                    print(f\"üîç Outliers detected: {total_outliers} ({outlier_pct:.1f}%)\")\n",
    "                    print(f\"   - Below lower bound: {n_outliers_low}\")\n",
    "                    print(f\"   - Above upper bound: {n_outliers_high}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\n‚ùå [Column {col_name}] Error during fitting: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"‚úÖ SmartOutlierHandler fitting completed successfully!\")\n",
    "            print(f\"üìã Processed {len(self.clip_params_)} columns\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X: Union[pd.DataFrame, np.ndarray]) -> np.ndarray:\n",
    "        \"\"\"Transform the data by handling outliers with proper dtype preservation.\"\"\"\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = check_array(X, ensure_2d=True, ensure_all_finite='allow-nan')\n",
    "        #X = check_array(X, ensure_2d=True)\n",
    "        if np.iscomplexobj(X):\n",
    "            raise ValueError(\"Complex data not supported\")\n",
    "        # Convert to DataFrame if not already\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(X)\n",
    "            \n",
    "        # Validate feature dimensions\n",
    "        if X.shape[1] != self.n_features_in_:\n",
    "            raise ValueError(\n",
    "                f\"X has {X.shape[1]} features, but SmartOutlierHandler is \"\n",
    "                f\"expecting {self.n_features_in_} features as input.\"\n",
    "            )\n",
    "            \n",
    "        # Create a copy to avoid modifying original data\n",
    "        X = X.copy() \n",
    "        \n",
    "        numeric_cols = [X.columns[col_idx] for col_idx in self.columns_ if col_idx < X.shape[1]]\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"üî¢ Processing {len(numeric_cols)} numeric columns\")\n",
    "            print(f\"üìã Columns: {numeric_cols}\")\n",
    "        \n",
    "        # Convert numeric columns to float for processing while keeping original values\n",
    "        original_values = {col: X[col].copy() for col in numeric_cols}\n",
    "        X[numeric_cols] = X[numeric_cols].astype(np.float64)\n",
    "        \n",
    "        # Process outliers\n",
    "        total_outliers = 0\n",
    "        for col_idx in self.clip_params_:\n",
    "            if col_idx >= X.shape[1]:\n",
    "                continue\n",
    "                \n",
    "            col_name = X.columns[col_idx]\n",
    "            col_values = X[col_name]\n",
    "            mask_notna = col_values.notna()\n",
    "            valid_vals = col_values[mask_notna]\n",
    "            \n",
    "            if len(valid_vals) == 0:\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nüìå [Column {col_name}] No non-null values to process\")\n",
    "                continue\n",
    "                \n",
    "            if self.verbose:\n",
    "                print(f\"\\nüîß Processing column: {col_name}\")\n",
    "                print(f\"üìä Non-null values: {len(valid_vals)}\")\n",
    "                \n",
    "            col_outliers = 0\n",
    "            \n",
    "            if self.method == 'gmm':\n",
    "                if self.verbose:\n",
    "                    print(\"üîÆ Using GMM method for outlier detection\")\n",
    "                clusters = self.clip_params_[col_idx]['model'].predict(\n",
    "                    valid_vals.values.reshape(-1, 1))\n",
    "                for idx, cluster_id in zip(valid_vals.index, clusters):\n",
    "                    val = valid_vals.loc[idx]\n",
    "                    clip_info = self.clip_params_[col_idx]['clip_info'][cluster_id]\n",
    "                    if val < clip_info['low'] or val > clip_info['high']:\n",
    "                        new_val = self._replace(val, valid_vals, \n",
    "                                             clip_info['low'], \n",
    "                                             clip_info['high'])\n",
    "                        X.loc[idx, col_name] = new_val\n",
    "                        col_outliers += 1\n",
    "            elif self.method == 'isolation_forest':\n",
    "                if self.verbose:\n",
    "                    print(\"üå≤ Using Isolation Forest for outlier detection\")\n",
    "                preds = self.clip_params_[col_idx]['model'].predict(\n",
    "                    valid_vals.values.reshape(-1, 1))\n",
    "                outliers_idx = valid_vals.index[preds == -1]\n",
    "                col_outliers = len(outliers_idx)\n",
    "                for idx in outliers_idx:\n",
    "                    val = valid_vals.loc[idx]\n",
    "                    bounds = self.clip_params_[col_idx]\n",
    "                    new_val = self._replace(val, valid_vals,\n",
    "                                         bounds['low'],\n",
    "                                         bounds['high'])\n",
    "                    X.loc[idx, col_name] = new_val\n",
    "            else:\n",
    "                bounds = self.clip_params_[col_idx]\n",
    "                if self.verbose:\n",
    "                    print(f\"üìè Using {'IQR' if self.method == 'iqr' else 'quantile'} bounds:\")\n",
    "                    print(f\"   Lower: {bounds['low']:.2f}, Upper: {bounds['high']:.2f}\")\n",
    "                \n",
    "                for idx in valid_vals.index:\n",
    "                    val = valid_vals.loc[idx]\n",
    "                    if val < bounds['low'] or val > bounds['high']:\n",
    "                        new_val = self._replace(val, valid_vals,\n",
    "                                             bounds['low'],\n",
    "                                             bounds['high'])\n",
    "                        X.loc[idx, col_name] = new_val\n",
    "                        col_outliers += 1\n",
    "            \n",
    "            total_outliers += col_outliers\n",
    "            if self.verbose:\n",
    "                outlier_pct = col_outliers / len(valid_vals) * 100\n",
    "                print(f\"üîç Outliers handled: {col_outliers} ({outlier_pct:.1f}%)\")\n",
    "                print(f\"üîÑ Replacement strategy: {self.replace_strategy}\")\n",
    "        \n",
    "        # Convert back to original dtypes carefully\n",
    "        for col in numeric_cols:\n",
    "            original_dtype = original_values[col].dtype\n",
    "            if pd.api.types.is_integer_dtype(original_dtype):\n",
    "                if X[col].isna().any():\n",
    "                    # Use pandas' nullable integer type if there are NaNs\n",
    "                    new_dtype = f'Int{original_dtype.itemsize*8}'\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nüîÑ Converting {col} to nullable integer type: {new_dtype}\")\n",
    "                    X[col] = X[col].round().astype(new_dtype)\n",
    "                else:\n",
    "                    # Round and convert back to original integer type\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nüîÑ Converting {col} back to original integer type: {original_dtype}\")\n",
    "                    X[col] = X[col].round().astype(original_dtype)\n",
    "            else:\n",
    "                # For non-integer types, convert directly\n",
    "                if self.verbose:\n",
    "                    print(f\"\\nüîÑ Converting {col} back to original dtype: {original_dtype}\")\n",
    "                X[col] = X[col].astype(original_dtype)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"‚úÖ Transformation completed successfully!\")\n",
    "            print(f\"üîç Total outliers handled: {total_outliers}\")\n",
    "            print(\"=\"*50 + \"\\n\")\n",
    "        \n",
    "        return X.values\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X, y).transform(X)\n",
    "\n",
    "    def _fit_gmm(self, series: pd.Series):\n",
    "        \"\"\"Fit Gaussian Mixture Model to detect clusters with robust enhancements.\"\"\"\n",
    "        data = series.values.reshape(-1, 1)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nüîç Starting GMM fitting process...\")\n",
    "            print(f\"üìä Data range: [{data.min():.2f}, {data.max():.2f}]\")\n",
    "        \n",
    "        # **üåü 1. Improved KDE peak detection**\n",
    "        if self.verbose:\n",
    "            print(\"üìà Computing Kernel Density Estimation...\")\n",
    "        kde = gaussian_kde(data.ravel())\n",
    "        x = np.linspace(data.min(), data.max(), 1000)\n",
    "        y = kde(x)\n",
    "        \n",
    "        # **üåü Better peak detection with prominence filtering**\n",
    "        if self.verbose:\n",
    "            print(\"üèîÔ∏è Detecting peaks in density estimation...\")\n",
    "        peaks, properties = find_peaks(y, \n",
    "                                     height=self.min_peak_height_ratio*y.max(),\n",
    "                                     prominence=0.1*y.max())  # **üåü Min prominence\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"üîç Found {len(peaks)} potential peaks in density\")\n",
    "        \n",
    "        # **üåü 2. Bayesian Information Criterion for component selection**\n",
    "        bic_scores = []\n",
    "        max_possible_components = min(len(peaks), self.max_components) or 1\n",
    "        n_components_range = range(1, max_possible_components + 1)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nüîÆ Testing different numbers of components:\")\n",
    "            print(f\"üî¢ Testing components: {list(n_components_range)}\")\n",
    "        \n",
    "        for n in n_components_range:\n",
    "            gmm = GaussianMixture(n_components=n, \n",
    "                                 covariance_type='full',  # **üåü Flexible shapes\n",
    "                                 random_state=42,\n",
    "                                 reg_covar=1e-6)  # **üåü Numerical stability\n",
    "            gmm.fit(data)\n",
    "            bic_scores.append(gmm.bic(data))\n",
    "            if self.verbose:\n",
    "                print(f\"   - Components={n}: BIC={gmm.bic(data):.2f}\")\n",
    "        \n",
    "        # **üåü Select model with lowest BIC**\n",
    "        best_n = n_components_range[np.argmin(bic_scores)]\n",
    "        best_gmm = GaussianMixture(n_components=best_n,\n",
    "                                  covariance_type='full',\n",
    "                                  random_state=42,\n",
    "                                  reg_covar=1e-6).fit(data)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"\\nüèÜ Selected model with {best_n} components (lowest BIC)\")\n",
    "            print(f\"üìä Component weights: {best_gmm.weights_}\")\n",
    "        \n",
    "        # **üåü 3. Cluster validation and filtering**\n",
    "        clusters = best_gmm.predict(data)\n",
    "        cluster_clip = {}\n",
    "        valid_clusters = 0\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"\\nüîç Validating clusters:\")\n",
    "        \n",
    "        for c in range(best_n):\n",
    "            cluster_data = data[clusters == c]\n",
    "            cluster_size = len(cluster_data)\n",
    "            \n",
    "            # **üåü Minimum cluster size requirement (5% of data)**\n",
    "            if cluster_size > max(5, 0.05 * len(data)):\n",
    "                low = np.quantile(cluster_data, self.border_quantile)\n",
    "                high = np.quantile(cluster_data, 1 - self.border_quantile)\n",
    "                cluster_clip[c] = {'low': low, 'high': high}\n",
    "                valid_clusters += 1\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(f\"   ‚úÖ Cluster {c}: size={cluster_size} ({cluster_size/len(data)*100:.1f}%)\")\n",
    "                    print(f\"      Bounds: [{low:.2f}, {high:.2f}]\")\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(f\"   ‚ùå Cluster {c}: too small (size={cluster_size}), skipping\")\n",
    "        \n",
    "        # **üåü Fallback to global quantiles if no valid clusters**\n",
    "        if valid_clusters == 0:\n",
    "            if self.verbose:\n",
    "                print(\"‚ö†Ô∏è No valid clusters found - using global quantiles\")\n",
    "            low = np.quantile(data, self.border_quantile)\n",
    "            high = np.quantile(data, 1 - self.border_quantile)\n",
    "            cluster_clip[0] = {'low': low, 'high': high}\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"   Global bounds: [{low:.2f}, {high:.2f}]\")\n",
    "        \n",
    "        return best_gmm, cluster_clip\n",
    "\n",
    "    def _replace(self, value: float, series: pd.Series, \n",
    "                low: float, high: float) -> float:\n",
    "        \"\"\"Replace outlier value according to strategy.\"\"\"\n",
    "        #if self.verbose: print(f\"   üîÑ Replacing outlier value {value:.2f} (bounds: [{low:.2f}, {high:.2f}])\")\n",
    "        \n",
    "        if self.replace_strategy == 'clip':\n",
    "            new_val = np.clip(value, low, high)\n",
    "            #if self.verbose: print(f\"      ‚úÇÔ∏è Clipped to: {new_val:.2f}\")\n",
    "            return new_val\n",
    "        elif self.replace_strategy == 'null':\n",
    "            #if self.verbose: print(\"      üö´ Replaced with NaN\")\n",
    "            return np.nan\n",
    "        elif self.replace_strategy == 'mean':\n",
    "            new_val = series.mean()\n",
    "            #if self.verbose: print(f\"      üìä Replaced with mean: {new_val:.2f}\")\n",
    "            return new_val\n",
    "        elif self.replace_strategy == 'median':\n",
    "            new_val = series.median()\n",
    "            #if self.verbose: print(f\"      üìä Replaced with median: {new_val:.2f}\")\n",
    "            return new_val\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown replace strategy: {self.replace_strategy}\")\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        \"\"\"Get output feature names.\"\"\"\n",
    "        check_is_fitted(self)\n",
    "        if input_features is None:\n",
    "            if hasattr(self, 'feature_names_in_'):\n",
    "                return self.feature_names_in_\n",
    "            return np.array([f\"feature_{i}\" for i in range(self.n_features_in_)])\n",
    "        return np.asarray(input_features, dtype=object)\n",
    "\n",
    "    @classmethod\n",
    "    def build_full_pipeline(cls, data: pd.DataFrame, \n",
    "                           numeric_cols: List[str] = None,\n",
    "                           method: str = 'iqr',\n",
    "                           replace_strategy: str = 'clip',\n",
    "                           **kwargs) -> Pipeline:\n",
    "        \"\"\"\n",
    "        Build complete pipeline with imputation and outlier handling.\n",
    "        \"\"\"\n",
    "        if numeric_cols is None:\n",
    "            numeric_cols = data.select_dtypes(include=np.number).columns.tolist()\n",
    "        \n",
    "        return Pipeline([\n",
    "            ('preprocessor', ColumnTransformer([\n",
    "                ('numeric', Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='median')),\n",
    "                    ('outlier', cls(\n",
    "                        columns=numeric_cols,\n",
    "                        method=method,\n",
    "                        replace_strategy=replace_strategy,\n",
    "                        **kwargs\n",
    "                    ))\n",
    "                ]), numeric_cols),\n",
    "                ('passthrough', 'passthrough', \n",
    "                 list(set(data.columns) - set(numeric_cols)))\n",
    "            ], remainder='passthrough'))\n",
    "        ])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28951de4-3ff9-46ec-bb60-f309383a8af5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
